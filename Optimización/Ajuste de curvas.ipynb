{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ajuste de curvas\n",
    "\n",
    "<img style=\"float: right; margin: 0px 0px 15px 15px;\" src=\"https://upload.wikimedia.org/wikipedia/commons/a/a8/Regression_pic_assymetrique.gif\" width=\"400px\" height=\"125px\" />\n",
    "\n",
    "> El **ajuste de curvas** es el proceso de construir una curva (función), que sea el mejor ajuste a una serie de puntos. Las curvas ajustadas pueden ser usadas como asistencia en la visualización de datos, para inferir valores de una función donde no hay datos disponibles, y para resumir la relación entre variables.\n",
    "\n",
    "**Referencia**:\n",
    "- https://en.wikipedia.org/wiki/Curve_fitting\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Introducción\n",
    "\n",
    "Consideremos un polinomio de grado uno:\n",
    "\n",
    "$$y = \\beta_1 x + \\beta_0.$$\n",
    "\n",
    "Esta es una **línea recta** que tiene pendiente $\\beta_1$. Sabemos que habrá una línea conectando dos puntos cualesquiera. Por tanto, *una ecuación polinómica de primer grado es un ajuste perfecto entre dos puntos*.\n",
    "\n",
    "Si consideramos ahora un polinomio de segundo grado,\n",
    "\n",
    "$$y = \\beta_2 x^2 + \\beta_1 x + \\beta_0,$$\n",
    "\n",
    "este se ajustará exactamente a tres puntos. Si aumentamos el grado de la función a la de un polinomio de tercer grado, obtenemos:\n",
    "\n",
    "$$y = \\beta_3 x^3 + \\beta_2 x^2 + \\beta_1 x + \\beta_0,$$\n",
    "\n",
    "que se ajustará a cuatro puntos.\n",
    "\n",
    "**Ejemplos**\n",
    "1. Encontrar la línea recta que pasa exactamente por los puntos $(3,1)$ y $(2,0)$.\n",
    "2. Encontrar la parábola que pasa exactamente por los puntos $(0,1)$, $(1,0)$ y $(2,1)$.\n",
    "\n",
    "**Solución**\n",
    "1. Consideramos $y=\\beta_1 x + \\beta_0$. Evaluando en el punto $(3,1)$, obtenemos $\\beta_1(3) + \\beta_0 = 1$. Ahora, evaluando en el punto $(2,0)$, obtenemos $\\beta_1(2) + \\beta_0 = 0$. De esta manera,\n",
    "$$\\left[\\begin{array}{cc} 1 & 3 \\\\ 1 & 2\\end{array}\\right]\\left[\\begin{array}{c} \\beta_0 \\\\ \\beta_1\\end{array}\\right]=\\left[\\begin{array}{c} 1 \\\\ 0\\end{array}\\right].$$\n",
    "Resolviendo, $\\beta_0=-2, \\, \\beta_1=1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Consideramos $y=\\beta_2 x^2 + \\beta_1 x + \\beta_0$. Evaluando en el punto $(0,1)$, obtenemos $\\beta_2(0)^2 + \\beta_1(0) + \\beta_0 = 1$. Ahora, evaluando en el punto $(1,0)$, obtenemos $\\beta_2(1)^2 + \\beta_1(1) + \\beta_0 = 0$. Finalmente, evaluando en el punto $(2,1)$, obtenemos $\\beta_2(2)^2 + \\beta_1(2) + \\beta_0 = 1$. De esta manera,\n",
    "$$\\left[\\begin{array}{ccc} 1 & 0 & 0 \\\\ 1 & 1 & 1 \\\\ 1 & 2 & 4 \\end{array}\\right]\\left[\\begin{array}{c} \\beta_0 \\\\ \\beta_1 \\\\ \\beta_2 \\end{array}\\right]=\\left[\\begin{array}{c} 1 \\\\ 0 \\\\ 1 \\end{array}\\right].$$\n",
    "Resolviendo, $\\beta_0=1, \\, \\beta_1=-2$ y $\\beta_2=1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ¿Qué tienen en común los anteriores problemas?\n",
    "Las curvas están completamente determinadas por los puntos (datos limpios, suficientes y necesarios).\n",
    "\n",
    "Esto se traduce en que, al llevar el problema a un sistema de ecuaciones lineales, existe una única solución: **no hay necesidad, ni se puede optimizar nada**.\n",
    "\n",
    "¿Tendremos datos así de **'*bonitos*'** en la vida real?\n",
    "\n",
    "La realidad es que los datos que encontraremos en nuestra vida profesional se parecen más a esto..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ¿Cómo ajustamos una curva a esto?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Problema básico\n",
    "\n",
    "<img style=\"float: right; margin: 0px 0px 15px 15px;\" src=\"https://upload.wikimedia.org/wikipedia/commons/3/3a/Linear_regression.svg\" width=\"400px\" height=\"125px\" />\n",
    "\n",
    "Consideramos que tenemos un conjunto de n pares ordenados de datos $(x_i,y_i)$, para $i=1,2,3,\\dots,n$.\n",
    "\n",
    "### ¿Cuál es la recta que mejor se ajusta a estos datos?\n",
    "Consideramos entonces ajustes de la forma $\\hat{f}(x) = \\beta_0+\\beta_1 x = \\left[1 \\quad x\\right]\\left[\\begin{array}{c} \\beta_0 \\\\ \\beta_1 \\end{array}\\right]=\\left[1 \\quad x\\right]\\boldsymbol{\\beta}$ (lineas rectas).\n",
    "\n",
    "Para decir '*mejor*', tenemos que definir algún sentido en que una recta se ajuste *mejor* que otra.\n",
    "\n",
    "**Mínimos cuadrados**: el objetivo es seleccionar los coeficientes $\\boldsymbol{\\beta}=\\left[\\beta_0 \\quad \\beta_1 \\right]^\\top$, de forma que la función evaluada en los puntos $x_i$ i.e.($\\hat{f}(x_i)$) aproxime los valores correspondientes $y_i$.\n",
    "\n",
    "La formulación por mínimos cuadrados, encuentra los $\\boldsymbol{\\beta}=\\left[\\beta_0 \\quad \\beta_1 \\right]^T$ que minimiza\n",
    "$$\\sum_{i=1}^{n}(y_i-\\hat{f}(x_i))^2=\\sum_{i=1}^{n}(y_i-\\left[1 \\quad x_i\\right]\\boldsymbol{\\beta})^2=\\left|\\left|\\boldsymbol{y}-\\boldsymbol{X}\\boldsymbol{\\beta}\\right|\\right|^2,$$\n",
    "\n",
    "donde $\\boldsymbol{y}=\\left[y_1,\\quad\\cdots\\quad, y_n\\right]^\\top$, y $\\boldsymbol{X}=\\left[\\begin{array}{ccc}1 & x_1\\\\ \\vdots & \\vdots \\\\ 1 & x_n\\end{array}\\right].$ Esto es,\n",
    "\n",
    "$$\\boldsymbol{\\beta}^{ls} = \\arg \\min_{\\boldsymbol{\\beta}} \\left|\\left|\\boldsymbol{y}-\\boldsymbol{X}\\boldsymbol{\\beta}\\right|\\right|^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Ajuste polinomial\n",
    "\n",
    "Ahora, considere el siguiente conjunto de datos..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Regularización\n",
    "\n",
    "Vimos que la solución de mínimos cuadrados es:\n",
    "$$\\boldsymbol{\\beta}^{ls} = \\arg \\min_{\\boldsymbol{\\beta}} \\left|\\left|\\boldsymbol{y}-\\boldsymbol{X}\\boldsymbol{\\beta}\\right|\\right|^2.$$\n",
    "\n",
    "Sin embargo, si crecemos el orden del modelo hay overfitting y algunos coeficientes óptimos $\\boldsymbol{\\beta}$ crecen muchísimo. Que un coeficiente sea muy grande, significa que se le da mucha importancia a alguna característica (que quizá sea ruido... no sirve para predecir).\n",
    "\n",
    "La regularización consiste en penalizar la magnitud de los coeficientes $\\boldsymbol{\\beta}$ en el problema de optimización, para que no crezcan tanto. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Ridge\n",
    "\n",
    "$$\\boldsymbol{\\beta}^{ridge} = \\arg \\min_{\\boldsymbol{\\beta}} \\left|\\left|\\boldsymbol{y}-\\boldsymbol{X}\\boldsymbol{\\beta}\\right|\\right|^2 + \\lambda\\left|\\left|\\boldsymbol{\\beta}\\right|\\right|^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Lasso\n",
    "\n",
    "$$\\boldsymbol{\\beta}^{lasso} = \\arg \\min_{\\boldsymbol{\\beta}} \\left|\\left|\\boldsymbol{y}-\\boldsymbol{X}\\boldsymbol{\\beta}\\right|\\right|^2 + \\lambda\\left|\\left|\\boldsymbol{\\beta}\\right|\\right|_1$$\n",
    "\n",
    "La norma 1 no es más que la suma de los valores absolutos de las componentes $\\left|\\left|\\boldsymbol{\\beta}\\right|\\right|_1=\\sum_{j=0}^m\\left|\\beta_j\\right|$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
